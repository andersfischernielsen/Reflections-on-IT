\chapter{Introduction}
The idea of creating mass-produced self-driving, or autonomous, vehicle has seen more and more interest from researchers and car manufacturers in the last few years.

The idea of inventing an autonomous car is not a new thing with people having dreamt up future societies where people would focus on other things while their cars drove them from A to B\todo{cite. }. In the last decade these ideas are finally being realised with research projects such as Google's ongoing Self Driving Car Project, previous research projects such as the entries of Carnegie Melon and Stanfords in the DARPA Grand Challenge and the recent undertakings of most car manufacturers, such as BMW, Audi, Toyota, VW etc. \todo{cite.} 

\newpar Certain ethical issues arise with autonomous vehicles, however. Since autonomous vehicles are programmed to handle preferably every possible scenario on the road in advance, autonomous vehicles present ethical dilemmas that human drivers in non-autonomous vehicles do not. 

\newpar Human drivers are rarely blamed for acting according to their instinct in life-threatening vehicle collisions, even if this means that they inadvertently make decisions that have fatal consequences for other people. 

These decisions have to be programmed into autonomous vehicles in advance in order to make the "right" choice, should they occur. 
This raises interesting questions, such as; who should be blamed for an autonomous vehicle hitting, and possibly killing, one person over another? Should we even program autonomous vehicles to be able to select the "preferable" target of collision, or would it be better to not do any selection at all and make the vehicle choose some random behaviour in a critical situation?

The main question I will try to answer in this essay is; "Who should determine who your future autonomous car hits in an emergency situation?" by looking into what ethical issues this question presents, and what theories can help answer this question.  